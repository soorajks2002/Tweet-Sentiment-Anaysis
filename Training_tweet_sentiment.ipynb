{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeUyDfXaVcC0"
      },
      "source": [
        "# Import Required Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4HcPKrFVcC_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nYsxyfSVcDD"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tCaFKL-VcDE"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/twitter_train_col.csv')\n",
        "\n",
        "df.content = df.content.fillna(' ')\n",
        "\n",
        "text = df.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oOLUlb9VcDF"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDnDe63wWW7D",
        "outputId": "1ec1b9ec-a01e-4ea3-d267-f3432252f8db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Yo4nk4rVcDG"
      },
      "outputs": [],
      "source": [
        "uniq = []\n",
        "\n",
        "for a in text :\n",
        "    wor = re.sub('[^a-z A-Z]',' ',a)\n",
        "    wor = wor.lower()\n",
        "    wor = wor.split()\n",
        "\n",
        "    wor = [WordNetLemmatizer().lemmatize(word) for word in wor if not word in set(stopwords.words('english'))]\n",
        "\n",
        "    wor = ' '.join(wor)\n",
        "    uniq.append(wor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkeD1FvSVcDI"
      },
      "source": [
        "# Converting Text To Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwWNXjZVVcDK"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anbFflOtVcDL"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "x = vectorizer.fit_transform(uniq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwYS73BDVcDN"
      },
      "source": [
        "## Formating Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_JuBwWZVcDO"
      },
      "outputs": [],
      "source": [
        "y = df.sentiment\n",
        "\n",
        "y = y.replace(['Positive', 'Neutral', 'Negative', 'Irrelevant'],[1,0,2,3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T7bgorZVcDP"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3cJFzyLVcDQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iM_9FNiOVcDR"
      },
      "outputs": [],
      "source": [
        "model = MultinomialNB().fit(x,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "hznADtRcYFve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('/content/twitter_valid_col.csv')\n",
        "\n",
        "df_test.content = df_test.content.fillna(' ')\n",
        "\n",
        "valid = df_test.content"
      ],
      "metadata": {
        "id": "Ascw1eUvYI2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uniq_test = []\n",
        "\n",
        "for a in valid :\n",
        "    wor_test = re.sub('[^a-z A-Z]',' ',a)\n",
        "    wor_test = wor_test.lower()\n",
        "    wor_test = wor_test.split()\n",
        "\n",
        "    wor_test = [WordNetLemmatizer().lemmatize(word_test) for word_test in wor_test if not word_test in set(stopwords.words('english'))]\n",
        "\n",
        "    wor_test = ' '.join(wor_test)\n",
        "    uniq_test.append(wor_test)"
      ],
      "metadata": {
        "id": "Ppok8PLNYlIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = vectorizer.transform(uniq_test)"
      ],
      "metadata": {
        "id": "fXLJa81SY-mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = df_test.sentiment\n",
        "\n",
        "y_test = y_test.replace(['Positive', 'Neutral', 'Negative', 'Irrelevant'],[1,0,2,3])"
      ],
      "metadata": {
        "id": "8SgHVa87ZA_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(x_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIXyBBiBZDEr",
        "outputId": "8f3c7c5b-661d-4f33-c72e-2358f3583e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8048048048048048"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl4m3UTNVcDS"
      },
      "source": [
        "## Save Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8c_lPVWVcDS"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mw7Tjn5tVcDT"
      },
      "outputs": [],
      "source": [
        "pickle.dump(model, open('trained_model', 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPDXSWIfVcDT"
      },
      "source": [
        "## Save Text Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEAQEuo2VcDU"
      },
      "outputs": [],
      "source": [
        "pickle.dump(model, open('tweet_vectorizer', 'wb'))"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Training_tweet_sentiment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}